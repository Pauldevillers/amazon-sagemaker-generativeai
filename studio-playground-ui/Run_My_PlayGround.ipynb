{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7aa5e7",
   "metadata": {},
   "source": [
    "## Installation of Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711c999-fb77-48da-a372-684e9a721758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95f3fa-1444-45a0-8e9d-ccfad7c3df8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54511796-5e5d-401b-beb3-e37abb31eda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "region = boto3.Session().region_name\n",
    "client = boto3.Session().client(\"sagemaker\")\n",
    "domainId=client.list_domains()[\"Domains\"][0].get(\"DomainId\")\n",
    "sess= sagemaker.session.Session()\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "vpc_id = client.describe_domain(\n",
    "    DomainId=client.list_domains()[\"Domains\"][0].get(\"DomainId\")\n",
    ").get(\"VpcId\")\n",
    "\n",
    "url = f\"https://{domainId}.studio.{region}.sagemaker.aws/jupyter/default/proxy/8501/\"\n",
    "print(url )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44550dfc",
   "metadata": {},
   "source": [
    "# Generative AI Prompt example with Falcon7B instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767390e8",
   "metadata": {},
   "source": [
    "## Use Case : Report generation from structured data for Social networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa01f527",
   "metadata": {},
   "source": [
    "<p>Let's say you want to generate a report from structured data of social network users. You can once again utilize <b>few-shot prompting</b> to generate text based on predefined examples.\n",
    "\n",
    "In the example below, I have gathered fake JSON data about Twitter users along with their statistics.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3a2b05",
   "metadata": {},
   "source": [
    "```\n",
    "JSON\n",
    "{\n",
    "  \"username\": ‚ÄúPaul‚Äù,\n",
    "  \"platform\": \"Twitter\",\n",
    "  \"summary\": {\n",
    "    \"total_posts\": 150,\n",
    "    \"total_likes\": 1000,\n",
    "    \"total_retweets\": 500,\n",
    "    \"most_liked_post\": {\n",
    "      \"content\": ‚Äúrugby game‚Äù,\n",
    "      \"likes\": 200,\n",
    "      \"retweets\": 50\n",
    "    },\n",
    "    \"most_retweeted_post\": {\n",
    "      \"content\": ‚Äúcan‚Äôt wait to see France vs New Zealand‚Äù,\n",
    "      \"likes\": 150,\n",
    "      \"retweets\": 100\n",
    "    },\n",
    "    \"engagement_rate\": 0.15\n",
    "  }\n",
    "}\n",
    "\n",
    "Report: \n",
    "Meet Paul, an active Twitter user who knows how to make an impact! With 150 posts, he has garnered an impressive 1000 likes and 500 retweets. Paul's top-notch content shines through, as his post about a rugby game scored a whopping 200 likes and 50 retweets. Not stopping there, his tweet expressing excitement for the France vs New Zealand match drew attention with 150 likes and 100 retweets. Talk about sparking conversations! With an engagement rate of 0.15, Paul is clearly making waves on Twitter. Keep up the great work, Paul!\n",
    "\n",
    "JSON:\n",
    "{\n",
    "  \"username\": ‚ÄúIoan‚Äù,\n",
    "  \"platform\": \"Twitter\",\n",
    "  \"summary\": {\n",
    "    \"total_posts\":  4000,\n",
    "    \"total_likes\": 5000,\n",
    "    \"total_retweets\": 20,\n",
    "    \"most_liked_post\": {\n",
    "      \"content\": ‚ÄúGenAI in France‚Äù,\n",
    "      \"likes\": 2000,\n",
    "      \"retweets\": 500\n",
    "    },\n",
    "    \"most_retweeted_post\": {\n",
    "      \"content\": ‚Äúcan‚Äôt wait to see genAI immersion day‚Äù,\n",
    "      \"likes\": 1500,\n",
    "      \"retweets\": 100\n",
    "    },\n",
    "    \"engagement_rate\": 0.15\n",
    "  }\n",
    "}\n",
    "\n",
    "Report:\n",
    "Introducing Ioan, a Twitter enthusiast who knows how to make a splash! With a whopping 4000 posts, Ioan has managed to accumulate an impressive 5000 likes. While his retweet count stands at 20, Ioan's impact shines through his most liked post, \"GenAI in France,\" which received a staggering 2000 likes and 500 retweets. Additionally, his tweet expressing anticipation for \"genie immersion day\" gained traction with 1500 likes and 100 retweets. With an engagement rate of 0.15, Ioan's Twitter presence is undoubtedly making waves. Keep up the fantastic work, Ioan!\n",
    "\n",
    "JSON\n",
    "{\n",
    "  \"username\": ‚ÄúAlbert‚Äù,\n",
    "  \"platform\": \"Twitter\",\n",
    "  \"summary\": {\n",
    "    \"total_posts\":  40,\n",
    "    \"total_likes\": 5,\n",
    "    \"total_retweets\": 20,\n",
    "    \"most_liked_post\": {\n",
    "      \"content\": ‚ÄúDevops‚Äù,\n",
    "      \"likes\": 2,\n",
    "      \"retweets\": 5\n",
    "    },\n",
    "    \"most_retweeted_post\": {\n",
    "      \"content\": ‚Äúcan‚Äôt wait to swim‚Äù,\n",
    "      \"likes\": 15,\n",
    "      \"retweets\": 1\n",
    "    },\n",
    "    \"engagement_rate\": 0.01\n",
    "  }\n",
    "}\n",
    "\n",
    "Report:\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d109cc",
   "metadata": {},
   "source": [
    "## AWS expert chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce1f0c66",
   "metadata": {},
   "source": [
    "Let's say we are building a friendly chat bot to start on AWS, we can levarage Falcon7B.\n",
    "```\n",
    "You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ddd7c66",
   "metadata": {},
   "source": [
    "You can also try with anther user prompt:\n",
    "\n",
    "```\n",
    "- How would you recommend start using Amazon SageMaker? If i am new to Machine Learning?\n",
    "- How would you recommend start using Amazon S3? If i am new to Storage?\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e07abc1",
   "metadata": {},
   "source": [
    "##### This decoder only model predicts next tokens, hence it won't stop unless you give it a stop word, in this case we can set the stop word option to `User:`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c35fab",
   "metadata": {},
   "source": [
    "# Other use case "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3085b0f8",
   "metadata": {},
   "source": [
    "#### Text Generation\n",
    "- **Parameters** : max_new_tokens = 400 <p>\n",
    "```\n",
    "What is the recipe for a delicious lemon cheesecake?\n",
    "```\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b0192e9",
   "metadata": {},
   "source": [
    "#### Text Generation\n",
    "\n",
    "```\n",
    ">>QUESTION<< \n",
    "Building a website can be done in 10 simple steps\n",
    "\n",
    ">>ANSWER<<\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de1cfe00",
   "metadata": {},
   "source": [
    "### Code generation\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "878c4696",
   "metadata": {},
   "source": [
    "```\n",
    "You are tasked with creating a program that calculates the factorial of a given number. Write a Python function called factorial that takes an integer n as input and returns its factorial. The factorial of a non-negative integer n is the product of all positive integers less than or equal to n. For example, the factorial of 5 is calculated as 5 x 4 x 3 x 2 x 1 = 120.\n",
    "\n",
    "Your task is to generate the code for the factorial function. The function should include a docstring explaining its purpose and a parameter for the input integer n. It should use a loop to iteratively calculate the factorial and return the result. \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbff45c9",
   "metadata": {},
   "source": [
    "#### Summarize\n",
    "- **Parameters** : max_new_tokens = 200 <p>\n",
    "```\n",
    "Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "programmatically through the SageMaker Python SDK.\n",
    "Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "needs for applications such as translation, question answering, and summarizing information.\n",
    "Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart ‚Äì\n",
    "Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\n",
    "```\n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b08f3498",
   "metadata": {},
   "source": [
    "# Generative AI Prompt example with FLAN-T5-XXL instruct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbeed543",
   "metadata": {},
   "source": [
    "## Use case : Data extraction for Sport News"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9e2304",
   "metadata": {},
   "source": [
    "<p>Let's say you want to extract structured data from sport news. You can use <b>few shot prompting</b> to extract relevant information such as names, dates, numbers, ect </p> \n",
    "\n",
    "In the example below,I've collected news about soccer [transfer information](https://twitter.com/FabrizioRomano) and the goal is to extract a structured JSON document with :\n",
    "- Sport\n",
    "- Person involve in the transfer\n",
    "- Club\n",
    "- Money involve "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "feb6429d",
   "metadata": {},
   "source": [
    "```\n",
    "[Text] : West Ham have just communicated to Arsenal that they‚Äôre accepting ¬£100m plus ¬£5m add-ons fee for Declan Rice. The two clubs remain in talks over deal structure & payment terms ‚Äî as West Ham want ¬£100m to be paid within 18 months. Final discussions and then‚Ä¶ done deal.\n",
    "\n",
    "[JSON] : { \"sport\" : \"foot\", \"who\":\"Declan Rice\", \"club\" :\" West Ham\" ,\"money\":  \"¬£100m\" }\n",
    "\n",
    "[Text] : BREAKING: Manchester United agree ¬£60m package deal for Mason Mount with Chelsea ‚Äî it‚Äôs done, here we go! #MUFC Personal terms agreed weeks ago and face to face talks between clubs made it clear: the agreement is done. Mount becomes Utd player ‚Äî they NEVER left the race.\n",
    "\n",
    "\n",
    "[JSON] : {\"sport\" : \"foot\", \"who\": \"Mason Mount\", \"club:Manchester United\",  \"money\":   \"¬£60m\" }\n",
    "\n",
    "[Text] : James Maddison has completed first part of medical tests as new Tottenham player, as expected. . #THFC ¬£40m deal to be announced soon ‚Äî here we go confirmed.\n",
    "\n",
    "[JSON] :{\"sport\" :\" foot\", \"who\": \"James Maddison\" : \"club\": \"Tottenham\" , \"money\":   \"¬£60m\" }\n",
    "\n",
    "[Text] : Manchester United had new direct talks to be informed on Andr√© Onana deal in the last 24 hours. He‚Äôs highly rated by Erik ten Hag. üî¥üá®üá≤ #MUFC. Man Utd, still waiting to clarify David de Gea situation before deciding on official proposal.Inter want at least ‚Ç¨50/55m.\n",
    "\n",
    "\n",
    "[JSON] : {\"sport\" : \"foot\", \"who\": \"David De Gea/Andr√© Onana\"  , \"club\" : \"Manchester United\" , \"money\": \"‚Ç¨50/55\"}\n",
    "\n",
    "[Text] : Medical scheduled for Harry Winks to join Tottenham on permanent deal from  Leicester City for ¬£10m fee. üîµü¶ä #LCFC. Separated deal from Maddison but one more set to be completed.\n",
    "\n",
    "[JSON] :\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82bd4076",
   "metadata": {},
   "source": [
    "## Use case : Data extraction for online user orders "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2375be0e",
   "metadata": {},
   "source": [
    "<p>Let's say you want to extract structured data from user orders. You can use <b>Chain of thoughts (CoT) </b> to extract relevant information such as Price, Resolution, deliverDays </p> \n",
    "\n",
    "In the example below,I've collected user orders and the goal is to extract a structured JSON document with :\n",
    "- minimumPrice\n",
    "- maximumPrice\n",
    "- deliverInDays\n",
    "- resolution\n",
    "- sizeFrom\n",
    "- sizeTo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fce7d86",
   "metadata": {},
   "source": [
    "#### **Parameters** : max_new_tokens = 67 <p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ea949f1",
   "metadata": {},
   "source": [
    "```\n",
    "You are given the below typescript schema:\n",
    "\n",
    "type Resolution = '8K' | '4K' | '1080p' | '720p'\n",
    "\n",
    "interface TvSearchQuery {\n",
    "    minimumPrice?: Number;\n",
    "    maximumPrice?: Number;\n",
    "    deliverInDays?: Number;\n",
    "    resolution?: Resolution;\n",
    "    sizeFrom?: Number;\n",
    "    sizeTo?: Number;\n",
    "}\n",
    "\n",
    "Analyze user's asking, and then using this documentation, generate a complete json\n",
    " object matching the schema for answering the user's asking. Prefer simpler searches\n",
    " unless specified in the user's question.\n",
    " \n",
    " \n",
    "[Asking]: I want to order a TV with 55 inches or bigger, delivered within 2 days. My budget is $2000.\n",
    "\n",
    "[Answer]: size is from 55. deliverInDays is 2. price is up to 2000. Generated json object is { \"maximumPrice\": 2000, \"deliverInDays\": 2, \"sizeFrom\": 55 }\n",
    "\n",
    "\n",
    "[Asking] : I want 8K or 4K TV under $3000.\n",
    "\n",
    "[Answer] : resolution is 8K or 4K. price is up to 3000. Generated json object is { \"maximumPrice\": 3000, \"resolution\": [\"8K\", \"4K\"] }\n",
    "\n",
    "\n",
    "[Asking] : a 4K TV needed in 3 days.\n",
    "\n",
    "[Answer] : price is not specified. resolution is 4K. deliverInDays is 3. Generated json object is\n",
    "{ \"deliverInDays\": 3, \"resolution\": [\"4K\"] }\n",
    "\n",
    "\n",
    "[Asking] : a 1080p TV needed in 5 days.\n",
    "\n",
    "[Answer] : price is not specified. resolution is 1080p. deliverInDays is 5. Generated json object is { \"deliverInDays\": 5, \"resolution\": [\"1080p\"] }\n",
    "\n",
    "\n",
    "[Asking] : I want 1080p or 4K TV over $500. I want it in 3 days.\n",
    "\n",
    "[Answer] : \n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88516125",
   "metadata": {},
   "source": [
    "### Supported parameters\n",
    "\n",
    "***\n",
    "Some of the supported parameters while performing inference are the following:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "For more parameters and information on HF LLM DLC, please see [this article](https://huggingface.co/blog/sagemaker-huggingface-llm#4-run-inference-and-chat-with-our-model).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ab551",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
