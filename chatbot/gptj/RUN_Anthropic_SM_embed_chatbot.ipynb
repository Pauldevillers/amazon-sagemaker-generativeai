{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486225e8-12a5-499f-a620-ea41ccef1815",
   "metadata": {},
   "source": [
    "# Langchain on SageMaker.\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augement the Large Models and enable us to perform tasks which meet our goals and enable our use-cases. At a high level Langchain has \n",
    "\n",
    "Data: Connect a language model to other sources of data\n",
    "Agent: Allow a language model to interact with its environment\n",
    "\n",
    "LangChain can be used in two major ways:\n",
    "\n",
    "<li>Indivisual Components: LangChain provides modular abstractions for the components neccessary to work with language models. LangChain also has collections of implementations for all these abstractions. The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.\n",
    "\n",
    "<li>Use-Case Specific Chains: Chains can be thought of as assembling these components in particular ways in order to best accomplish a particular use case. These are intended to be a higher level interface through which people can easily get started with a specific use case. These chains are also designed to be customizable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4789df3c-aff9-4957-a32d-80086b1f7ddb",
   "metadata": {},
   "source": [
    "## Topics covered:\n",
    "\n",
    "In this notebook we will be covering the below topics:\n",
    "\n",
    "**LLM** Examine running an LLM in bare form to check for output\n",
    "\n",
    "**Vector DB** Examine various vector databases like FAISS or CHROMA and leverage to produce better results using RAG\n",
    "\n",
    "**Prompt template** Examine use of PROMPT Template\n",
    "\n",
    "**Chatbot** Build a Interactive Chatbot with Memory "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f1176e9-9a60-4713-b72f-9e54d2a259b8",
   "metadata": {},
   "source": [
    "## Key points for consideration\n",
    "\n",
    "1. Long Document that exceed the token limit? Ability to Chain , Mapo_reduce, Refine, Map-Rerank\n",
    "2. Cost of per token -- minimize the tokens and send in only relevant tokens to Model\n",
    "3. Which model to use --\n",
    "    - Cohere, AI21, Huggingface Hub, Azure OpenAI, Manifest, Goose AI, Writer, Banana, Modal, StochasticAI, Cerebrium, Petals, Forefront AI, PromptLayer OpenAI, Anthropic, DeepInfra, and self-hosted Models.\n",
    "    - Example LLM cohere = Cohere(model='command-xlarge')\n",
    "    - Example LLM flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "4. Input Data Sources PDF, WebPages, CSV , S3, EFS\n",
    "5. Orchestration with External Tasks\n",
    "    - External Tasks - Agent SerpApi, SEARCH Engines\n",
    "    - Math Calculator\n",
    "6. Conversation Management and History"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de785d0-3b27-4699-87be-a34484c429fa",
   "metadata": {},
   "source": [
    "### Key components of LangChain\n",
    "\n",
    "Let us examine the key components of Langchain. At the heart and the center is the Large Model.\n",
    "\n",
    "There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:\n",
    "\n",
    "**Models**: The various model types and model integrations LangChain supports.\n",
    "\n",
    "<img src='./images/models.png' width =\"300\"/>\n",
    "\n",
    "    \n",
    "**Prompts**: This includes prompt management, prompt optimization, and prompt serialization.\n",
    "    \n",
    "<img src=\"images/prompt.png\" width=\"300\"/>\n",
    "    \n",
    "**Memory**: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "    \n",
    "**Indexes**: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\n",
    "    \n",
    "<img src=\"images/vectorstore.png\" width=\"300\"/>\n",
    "\n",
    "**Chains**: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "<img src=\"images/chains.png\" width=\"300\"/>\n",
    "\n",
    "**Agents**: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
    "\n",
    "\n",
    "    \n",
    "**Callbacks**: It can be difficult to track all that occurs inside a chain or agent. Callbacks help add a level of observability and introspection.\n",
    " \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "402714bf-14b6-4481-8e33-fc3d0b8a81f4",
   "metadata": {},
   "source": [
    "### Chat Bot key elements\n",
    "\n",
    "The first process in a chat bot is to generate embeddings. Typically you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using a GPT-J embeddings model for this\n",
    "\n",
    "<img src=\"images/Embeddings_lang.png\" width=\"300\"/>\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "<img src=\"images/Chatbot_lang.png\" width=\"300\"/>\n",
    "\n",
    "For processes which need deeper analysis, conversation history we will need to summarize every interaction to keep it succinct and for that we can follow this flow below which uses PineCone as an example\n",
    "\n",
    "For the various Tools which are available \n",
    "\n",
    "<img src=\"images/chatbot_internet.jpg\" width=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aed6880-101b-457a-9e99-25cc421ee8c5",
   "metadata": {},
   "source": [
    "# Pre-Requisites\n",
    "\n",
    "There are a few pre-reqs to be completed when running this notebook. The key one being setting up the LLM to be used.\n",
    "<li> Either have a FLAN-T5 model deployed in SageMaker using Lab5 at  at Deploy FlanT5-XXL from https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/generativeai/llm-workshop\n",
    "<li> Have Anthropic Model Key . You can choose to do both or either or . However certains cells might not work if you have just 1 and so you can choose to ignore those errors as part of the run\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af78b2be-9a3f-446a-b30e-597493664257",
   "metadata": {},
   "source": [
    "### LLM model is Anthropic \n",
    "\n",
    "follow the Notebook to get the Anthropic keys\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "### Install certain libraries which are needed for this run. \n",
    "\n",
    "These are provided in the requirements.txt or you can run these cells to fine control which libraries you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be60b-480a-4524-8a1d-3529ebcb812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.161 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8923e9-69f8-4561-8df3-8eca59c965fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install chromadb==0.3.21 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828474e-c07a-4dba-badd-068ac2ff6d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.161 boto3 html2text jinja2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50365364-45aa-4f80-b78e-89771afebc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu==1.7.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ebd16-11c0-446b-8b4c-547b1e956c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pypdf==3.8.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1b765-26a6-4bde-9491-31a2c804d26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.24.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae906df-a917-4b04-a36e-e8f184e6860f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers==2.2.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4ecc8-745e-466c-9799-da797baf00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d68f6-5a05-42d3-b3f4-bff698234e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "anthropic.ANTHROPIC_CLIENT_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2064f1-3cfa-4f19-b6cd-e14c7f16ec1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentence_transformers \n",
    "sentence_transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698a788-66e1-4409-9c90-0994cc072eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"all libraries installed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10d1542",
   "metadata": {},
   "source": [
    "#### Un comment the below and put in the appropriate values if you would like to use boto3 outside of Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166dea21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import os \n",
    "os.environ[\"ANTROPIC_KEY\"] = \"your_values\"\n",
    "\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<YOUR_VALUES>\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<YOUR_VALUES>\"\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<YOUR_VALUES>\"\n",
    "\n",
    "# os.environ['LANGCHAIN_ASSUME_ROLE'] = \"<YOUR_VALUES>\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f3a19c-78d7-4e60-9ba0-cec3c856ad1a",
   "metadata": {},
   "source": [
    "### Import statements for our chain and indexers. We are not using any explicit agent here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a816c-b828-4d6d-9bc1-ecd0c29ddc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from aws_langchain.kendra_index_retriever import KendraIndexRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67664670-35af-4561-a8af-1eb5967bd382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "#sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "#bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "# model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "# region = sess._region_name\n",
    "# account_id = sess.account_id()\n",
    "\n",
    "# s3_client = boto3.client(\"s3\")\n",
    "# sm_client = boto3.client(\"sagemaker\")\n",
    "# smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# jinja_env = jinja2.Environment()\n",
    "\n",
    "# print(region)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f60eb1-c3e5-41c8-ac57-e9db8123f74f",
   "metadata": {},
   "source": [
    "### Use a GPT-J sagemaker  embeddings Model - so we can use that to generate the embeddings for the documents\n",
    "\n",
    "\n",
    "This will be used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) and needs a g5.24xlarge instance to run\n",
    "\n",
    "Other Embeddings posible are here. [LangChain Embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786dd885-b173-4bed-977e-8492ada4e6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler, SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "_MODEL_CONFIG_ = {\n",
    "    \"huggingface-textembedding-gpt-j-6b\": {\n",
    "        \"instance type\": \"ml.g5.24xlarge\",\n",
    "        \"env\": {\"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "## -- used to deploy to a g5.24xlarge instance\n",
    "\n",
    "# - Uncomment and set these values in case you have an instance of GPT-J deployed already \n",
    "model_id = \"huggingface-textembedding-gpt-j-6b\"\n",
    "_MODEL_CONFIG_[model_id][\"endpoint_name\"] = 'RAGResearch-huggingface-textembedding-g-2023-04-21-18-15-41-742'  \n",
    "print( f'24xlarge::{_MODEL_CONFIG_[model_id][\"endpoint_name\"]}')\n",
    "\n",
    "\n",
    "sagemaker.image_uris.retrieve(\n",
    "        region=None,\n",
    "        framework=None,  # automatically inferred from model_id\n",
    "        image_scope=\"inference\",\n",
    "        model_id='huggingface-textembedding-gpt-j-6b',\n",
    "        model_version='*',\n",
    "        instance_type='ml.g5.24xlarge',\n",
    "    )\n",
    "\n",
    "model_version = \"*\"\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "for model_id in _MODEL_CONFIG_:\n",
    "    if not _MODEL_CONFIG_[model_id][\"endpoint_name\"]: # model has been deployed already \n",
    "        endpoint_name = sagemaker.utils.name_from_base(f\"RAGResearch-{model_id}\")\n",
    "        inference_instance_type = _MODEL_CONFIG_[model_id][\"instance type\"]\n",
    "        print(endpoint_name)\n",
    "\n",
    "        # Retrieve the inference container uri. This is the base HuggingFace container image for the default model above.\n",
    "        deploy_image_uri = sagemaker.image_uris.retrieve(\n",
    "            region=None,\n",
    "            framework=None,  # automatically inferred from model_id\n",
    "            image_scope=\"inference\",\n",
    "            model_id=model_id,\n",
    "            model_version=model_version,\n",
    "            instance_type=inference_instance_type,\n",
    "        )\n",
    "        # Retrieve the model uri.\n",
    "        model_uri = model_uris.retrieve(\n",
    "            model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    "        )\n",
    "        model_inference = Model(\n",
    "            image_uri=deploy_image_uri,\n",
    "            model_data=model_uri,\n",
    "            role=role,\n",
    "            predictor_cls=Predictor,\n",
    "            name=endpoint_name,\n",
    "            env=_MODEL_CONFIG_[model_id][\"env\"],\n",
    "        )\n",
    "        model_predictor_inference = model_inference.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=inference_instance_type,\n",
    "            predictor_cls=Predictor,\n",
    "            endpoint_name=endpoint_name,\n",
    "        )\n",
    "        print(f\"{bold}Model {model_id} has been deployed successfully.{unbold}{newline}::endpoint_name={endpoint_name}::\")\n",
    "        _MODEL_CONFIG_[model_id][\"endpoint_name\"] = endpoint_name\n",
    "    else:\n",
    "        print(f\"{bold}Model {model_id} has been Alreadyy deployed successfully.{unbold}{newline}Endpoint_name={_MODEL_CONFIG_[model_id]['endpoint_name']}::\")\n",
    "\n",
    "assumed_role = os.getenv('LANGCHAIN_ASSUME_ROLE', None)\n",
    "print(assumed_role)\n",
    "boto3_kwargs = {}\n",
    "session = boto3.Session()\n",
    "if assumed_role:\n",
    "    sts = session.client(\"sts\")\n",
    "    response = sts.assume_role(\n",
    "        RoleArn=str(assumed_role),\n",
    "        RoleSessionName=\"langchain-llm-1\"\n",
    "    )\n",
    "    print(response)\n",
    "    boto3_kwargs = dict(\n",
    "        aws_access_key_id=response['Credentials']['AccessKeyId'],\n",
    "        aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n",
    "        aws_session_token=response['Credentials']['SessionToken']\n",
    "    )\n",
    "\n",
    "boto3_sm_client = boto3.client(\n",
    "    \"sagemaker-runtime\",\n",
    "    **boto3_kwargs\n",
    ")\n",
    "print(boto3_sm_client)\n",
    "\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsLMI(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print()\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandlerEmbdSM(EmbeddingsContentHandler): #ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        #input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(f\"EMBEDDINGS::RESPONSE:{response_json}::\")\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        print(f\"EMBEDDINGS::RESPONSE::len[0]:{len(embeddings[0])}::current shape -- > {np.array(embeddings).shape}:: shape after unsqueeze -- > {np.array([embeddings]).shape}\")\n",
    "        if len(embeddings) == 1: # for the query embeddings - should be 1D vector because faiss will unsqueeze it \n",
    "            print(f\"EMBEDDINGS::returning:NO:SQUEEZE:: RESPONSE:{np.array(embeddings).shape}::\")\n",
    "            return embeddings #[0]\n",
    "        return embeddings # embeddings expected to be of shape 2D List[List[float]] -- >array 1 row with n dimensions\n",
    "\n",
    "\n",
    "assumed_role = os.getenv('LANGCHAIN_ASSUME_ROLE', None)\n",
    "print(assumed_role)\n",
    "boto3_kwargs = {}\n",
    "session = boto3.Session()\n",
    "if assumed_role:\n",
    "    sts = session.client(\"sts\")\n",
    "    response = sts.assume_role(\n",
    "        RoleArn=str(assumed_role), #\"arn:aws:iam::425576326687:role/SageMakerStudioDomainNoAuth-SageMakerExecutionRole-3RBLN6GPZ46O\",\n",
    "        RoleSessionName=\"langchain-llm-1\"\n",
    "    )\n",
    "    print(response)\n",
    "    boto3_kwargs = dict(\n",
    "        aws_access_key_id=response['Credentials']['AccessKeyId'],\n",
    "        aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n",
    "        aws_session_token=response['Credentials']['SessionToken']\n",
    "    )\n",
    "\n",
    "boto3_sm_client = boto3.client(\n",
    "    \"sagemaker-runtime\",\n",
    "    **boto3_kwargs\n",
    ")\n",
    "print(boto3_sm_client)\n",
    "content_handler_embd_sm = ContentHandlerEmbdSM()\n",
    "hf_embeddings = SagemakerEndpointEmbeddingsLMI(\n",
    "    client = boto3_sm_client,\n",
    "    endpoint_name=_MODEL_CONFIG_[\"huggingface-textembedding-gpt-j-6b\"][\"endpoint_name\"], #os.environ[\"FLAN_XXL_ENDPOINT\"],\n",
    "    region_name='us-east-1',\n",
    "    content_handler=content_handler_embd_sm,\n",
    ")\n",
    "hf_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a044aaa4-4d2b-44a8-aa7d-e690e08e4682",
   "metadata": {},
   "source": [
    "## Section 2: Use LangChain\n",
    "\n",
    "We will follow this pattern for the rest of the section\n",
    "\n",
    "<li>Exploring vector databases\n",
    "<li>Basics of QA exploring simple chains\n",
    "<li>Basics of chatbot\n",
    "<li>Going to prompt templates,\n",
    "<li>Exploring Chains\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d711c743-3d72-4c46-bcb4-6870f1d78c5e",
   "metadata": {},
   "source": [
    "### Exploring Vector DataBases and Create the Embeddings. \n",
    "\n",
    "Leverage SageMaker GPT-J model or the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f6618e7-646a-4db0-b6c5-d2a1642aa1f6",
   "metadata": {},
   "source": [
    "#### Use the file based document to retrieve based on embeddings\n",
    "\n",
    "Run the below to visualize the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aff84015-923b-4e8f-b89a-4543f6755210",
   "metadata": {},
   "source": [
    "#### Pull in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2b889-d6bd-4117-ad56-5dd5ef13d1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/\"\n",
    "\n",
    "!cd /home/ec2-user/SageMaker/univ-playground-ui/langchain && mkdir -p rag_data\n",
    "!cd /home/ec2-user/SageMaker/univ-playground-ui/langchain && aws s3 cp --recursive $original_data rag_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d18454-8d28-4662-a396-fb5f70730b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"rag_data/\", \"*.csv\"))\n",
    "\n",
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "#- drop \n",
    "df_answer = df_knowledge.drop([\"Question\"], axis=1)\n",
    "\n",
    "print(df_knowledge.shape)\n",
    "df_knowledge.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615a351-cb75-47f8-ba7c-1108934dae61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e90b0-7fa6-49d5-8fad-ffa7362c75be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e57a8df3-0285-4fed-ab51-5071058225cc",
   "metadata": {},
   "source": [
    "#### Create the embeddings for document search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e3a2e-1c4d-4892-b4d6-06f0fdc2f68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ad8c530-e385-4129-8912-1d2677c1b52c",
   "metadata": {},
   "source": [
    "#### Vector store indexer. \n",
    "\n",
    "This is what stores and matches the embeddings.This notebook showcases Chroma and FAISS and will be transient and in memory. The VectorStore Api's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "We will use our own Custom implementation of SageMaker Embeddings which needs a reference to the SageMaker endpoint to call the model which will return the embeddings. This will be used by the FAISS or Chroma to store in memory and be used when ever the User runs a query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffaae9b-0070-460b-a095-7680dd5ca6cc",
   "metadata": {},
   "source": [
    "#### Now, we see how simple it is to use LangChain to achieve question and answering application with just few lines of code. \n",
    "\n",
    "Let's break down the above VectorstoreIndexCreator and see what's happening under the hood. Furthermore, we will see how to incorporate a customize prompt rather than using a default prompt with VectorstoreIndexCreator.\n",
    "\n",
    "Firstly, we generate embedings for each of document in the knowledge library with SageMaker  embedding model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9d0b3da-f5ca-435a-a244-8cc1bedeb1e9",
   "metadata": {},
   "source": [
    "## Create the Anthropic Model\n",
    "\n",
    "This is the Large Model which will be running the summarization of the relevant documents based on the user query for the chat bot application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fabc3-e5ee-452b-b8b8-c91195d07ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "api_key = os.environ[\"ANTROPIC_KEY\"]\n",
    "sm_llm = ChatAnthropic(anthropic_api_key=api_key)\n",
    "print(f\"Antropic LLM model create={sm_llm}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89c314cc-d05d-425d-81ad-159eeb2d3c5b",
   "metadata": {},
   "source": [
    "#### Load the Data from our Documents Source. \n",
    "\n",
    "Then we will feed this into the VectorStore to create the embeddings using the loaders like [here](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html). First we will try with the SageMaker FAQ PDF document and also the IRS PDF files\n",
    "\n",
    "we will create 3 Loaders and 3 documents after doing a split on them. 1st loader for amazon faq, 2nd for some of the IRS PDF's, 3rd just for  some ramdom example. For text it will be just a separate loader, text loader vs pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20d1a0-cee5-45c8-be7f-a12f50191e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"rag_data/Amazon_SageMaker_FAQs.pdf\")\n",
    "documents_aws = loader.load() # -- gives 2 docs\n",
    "documents_split = loader.load_and_split() # - gives 22 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a41ba-ef70-4b57-bb55-586b3f2ab391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# - PDF 's  use load and split because that brings better results in search\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# all_files_pdf = all_files_pdf = glob.glob(\"rag_data/**/*.pdf\", recursive=True)\n",
    "# print(f\"All PDF files={len(all_files_pdf)}:\")\n",
    "\n",
    "# irs_files_pdf = glob.glob(\"rag_data/irs_zip/**/*.pdf\", recursive=True)\n",
    "# print(f\"IRS files files={len(all_files_pdf)}:\")\n",
    "\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# #- all pdfs\n",
    "# loader_allpdf = DirectoryLoader(\n",
    "#     \"./rag_data/\", \n",
    "#     glob = \"**/*.pdf\",\n",
    "#     loader_cls=PyPDFLoader,\n",
    "#     recursive=True\n",
    "# )\n",
    "# documents_all_pdf = loader_allpdf.load_and_split() # -- split when loading into FAISS\n",
    "\n",
    "# #- all IRS files\n",
    "# loader_irs = DirectoryLoader(\n",
    "#     \"./rag_data/irs_zip\", \n",
    "#     glob = \"*.pdf\",\n",
    "#     loader_cls=PyPDFLoader,\n",
    "#     recursive=True\n",
    "# )\n",
    "# documents_irs = loader_irs.load_and_split()\n",
    "\n",
    "# #- Example PDF \n",
    "# documents_example = PyPDFLoader(\"./materials/example.pdf\").load_and_split()\n",
    "\n",
    "# #- AMAZON AWS PDF \n",
    "# loader_aws = PyPDFLoader(\"./rag_data/Amazon_SageMaker_FAQs.pdf\")\n",
    "# documents_aws = loader_aws.load_and_split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c99f2a61-9c05-4a22-af32-df533918d719",
   "metadata": {},
   "source": [
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    CharacterTextSplitter(chunk_size=300, chunk_overlap=0).split_documents(documents_aws), \n",
    "    hf_embeddings, \n",
    "    #k=1\n",
    "    #**k_args\n",
    ")#### VectorStore as FAISS \n",
    "\n",
    "You can read up about [FAISS](https://arxiv.org/pdf/1702.08734.pdf) in memory vector store here. However for our example it will be the same \n",
    "\n",
    "Chroma\n",
    "\n",
    "[Chroma](https://www.trychroma.com/) is a super simple vector search database. The core-API consists of just four functions, allowing users to build an in-memory document-vector store. By default Chroma uses the Hugging Face transformers library to vectorize documents.\n",
    "\n",
    "Weaviate\n",
    "\n",
    "[Weaviate](https://github.com/weaviate/weaviate) is a very posh looking tool - not only does Weaviate offer a GraphQL API with support for vector search. It also allows users to vectorize their content using Weaviate's inbuilt modules or custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86402e-da63-4d2b-b477-c7fdc48812ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "k_args = {\"k\": 1}\n",
    "# - sub_docs = self.text_splitter.split_documents(docs)\n",
    "# - create Vectorstore\n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    CharacterTextSplitter(chunk_size=300, chunk_overlap=0).split_documents(documents_aws), \n",
    "    hf_embeddings, \n",
    "    #k=1\n",
    "    #**k_args\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "218075ec-587e-4c1a-b99d-d882d264b377",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### First way of running the Query. High Level abstraction\n",
    "\n",
    "Leverage VectorStoreIndexCreator which wraps around the RetrievalQA and provides a high level API abstraction to generate the response. This is a wrapper around the underlying API's which we will explore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9af51-fe45-494e-afe1-a26a0ce0798c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query=\"Simplified method for business use of home deduction\"\n",
    "query=\"What is SageMaker Spot Instances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825562b-295c-48dc-8e48-4bb17f686fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrapper_store_faiss.query(question=\"What is SageMaker Spot Instances\",llm=sm_llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8cc84bd-bc99-4840-80ee-9cbc424fedf7",
   "metadata": {},
   "source": [
    "##### Visualize Manually what is going on \n",
    "\n",
    "\n",
    "First we get the relevant documents based on the query by using the embeddings using the LLM summarize the outputs. These docs can be fed into the LLM to summarize and predict the answer. Here we can specify search type 'similiarity or Relevant' and K param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09559113-a106-4638-b278-6d33174d602e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "result_docs = wrapper_store_faiss.query_with_sources(\n",
    "    question=\"What is Amazon SageMaker Managed Spot Instances?\",\n",
    "    llm=sm_llm,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "print(result_docs)\n",
    "\n",
    "# - or you can use similiarity scores\n",
    "retriever = vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8})\n",
    "relevant_docs = retriever.get_relevant_documents(query)   \n",
    "print(len(relevant_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ead7f7b4-def1-4f3d-a89a-cebef5103f12",
   "metadata": {},
   "source": [
    "##### As a quick Test -- to do it manually Now invoke the LLM end point and feed the docs along with the query\n",
    "\n",
    "The results still will not come close to the answer we are expecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a4bd1-463e-4c17-b79d-9214edca61ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"Summarize this {relevant_docs} \"\n",
    "sm_llm = ChatAnthropic(anthropic_api_key=api_key)\n",
    "print(f\"Question being asked is -- > {query}:\")\n",
    "sm_llm.call_as_llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19949609-e6f5-4537-9d1f-9f9bdea42b44",
   "metadata": {},
   "source": [
    "## Exploring Chains and Prompt templates\n",
    "IN this section we will look at the cvarious flavors of chains and prompt templates\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31906bd6-a1cf-4699-8d8c-7ef9220fcd7e",
   "metadata": {},
   "source": [
    "#### Define a Chain\n",
    "\n",
    "[Chains](https://python.langchain.com/en/harrison-docs-refactor-3-24/modules/chains.html)  are the key to having a conversation in a chatbot manner. Here we will test **MANUALLY** injecting the documents retrived by doing a similiarity search. The final result matches our previous results in any case\n",
    "\n",
    "**Simplest QA Chain with NO CONTEXT being passed**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06015dc7-26ee-4354-8e02-3ada40baa267",
   "metadata": {},
   "source": [
    "#### PromptTemplate \n",
    "\n",
    "This can be enhanced by using a prompt template. More details  [PROMPT Template](https://python.langchain.com/en/harrison-docs-refactor-3-24/modules/prompts/prompt_templates.html)  \n",
    "\n",
    "We will start with a simple Chain and build up from there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49f66c-fb02-48fe-8b5c-5b0f79e7aa9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - assume a chat bot asks a question\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "  The following is a friendly conversation between a human and an AI. \n",
    "  The AI is talkative and provides lots of specific details from its context.\n",
    "  If the AI does not know the answer to a question, it truthfully says it \n",
    "  does not know.\n",
    "  {context}\n",
    "  Instruction: Based on the above documents, provide a detailed answer for, {question} Answer \"don't know\" if not present in the document. Solution:\n",
    "  \"\"\"\n",
    "PROMPT_T = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "PROMPT_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e236c9-e5dd-4085-8ae9-9567c639c715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## -- Load and run the Chain based on the prompt\n",
    "query=\"What is Amazon Managed SageMaker Spot Instances?\"\n",
    "\n",
    "# - increasing he search to 8 relevant documents does not bring great results with the embedding model we have\n",
    "relevant_docs = vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 3}).get_relevant_documents(query)   \n",
    "print(len(relevant_docs))\n",
    "chain = load_qa_chain(llm=sm_llm, prompt=PROMPT_T)\n",
    "result = chain({\"input_documents\": relevant_docs, \"question\": query}, return_only_outputs=True)\n",
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5f1443-028a-48d4-807c-f0de7b677b4d",
   "metadata": {},
   "source": [
    "#####  LLM Chain is another flavour for a simple chain. In reality you will be using a combination of few different chains as we will see in the chatbot section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e76361-8c1f-46f2-b238-08e5a08b213c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "query=\"What is Amazon SageMaker Managed Spot Instances?\"\n",
    "chain_t = LLMChain(llm=sm_llm, prompt=PROMPT_T)\n",
    "## -- Invoke the Chain ( call LLM ) to generate the Response\n",
    "result = chain_t({\"context\": relevant_docs, \"question\": query}, return_only_outputs=True)\n",
    "print(query)\n",
    "result['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ad6b08-2035-4828-a332-3784bb7d8075",
   "metadata": {},
   "source": [
    "#### With LangChain we do not need to manage this explictly and starting point is a RetrievalQA chain \n",
    "RetrievalQA chain which uses the load_qa_chain under the hood and here we retrieve the most relevant chunk of text and feed those into the language model. Below shows how it works. In most situations we will be using the complex chains by using the Chain module to get the results based on the query by the user. We use the RetrievalQA and pass in the Vector Store to get the same results\n",
    "\n",
    "However the results do not yet match our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b01db4-c3cb-4a53-93ee-24b581f8ec0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=sm_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8})\n",
    "    # - k of 8 brings 32k chars which is more than what our LLM can handle\n",
    ")\n",
    "\n",
    "#query=\"Simplified method for business use of home deduction\"\n",
    "query=\"What is Amazon SageMaker Managed Spot Instances?\"\n",
    "result = qa.run(query)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "791f09b8-b380-4988-bf2c-529db7f76538",
   "metadata": {},
   "source": [
    "#### Retrieval QA Chain\n",
    "\n",
    "BETTER Results with VectorRun using the QA chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf8c6b-c008-4b3c-8a89-754e91ff8001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qa_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=sm_llm, \n",
    "    chain_type=\"refine\", \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 3})\n",
    ")\n",
    "#query = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "result = qa_prompt.run(query)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e5cdc0",
   "metadata": {},
   "source": [
    "## Chatbot application\n",
    "\n",
    "#### For the chatbot we need context management, history, vector stores, and many other things. We will start by with a ConversationalRetrievalChain\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain which Allow for passing in chat history which can be used for follow up questions.Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "\n",
    "Set verbose to True to see all the what is going on behind the scenes\n",
    "\n",
    "**We use Custom Prompt template to fine tune the output responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42367663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \n",
    "\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "sm_llm = ChatAnthropic(anthropic_api_key=api_key)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=sm_llm, \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 3}),\n",
    "    memory=memory_chain,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, # use the condense prompt template\n",
    "    chain_type='refine',\n",
    "    max_tokens_limit=100\n",
    "    #combine_docs_chain_kwargs=key_chain_args,\n",
    "\n",
    ")\n",
    "print(\"Starting chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55c15c-9dc0-47ff-b559-05390cbf51e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \n",
    "\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "sm_llm = ChatAnthropic(anthropic_api_key=api_key)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=sm_llm, \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 3}),\n",
    "    memory=memory_chain,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, # use the condense prompt template\n",
    "    #chain_type='refine',\n",
    "    max_tokens_limit=100\n",
    "    #combine_docs_chain_kwargs=key_chain_args,\n",
    "\n",
    ")\n",
    "print(\"Starting chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7af2eaa-0e57-41e9-9283-a990c70e986b",
   "metadata": {},
   "source": [
    "#### Leverage ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc633580-53c2-4402-a4d2-797ab5245c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import ConversationChain\n",
    "from langchain.prompts import (\n",
    "  ChatPromptTemplate, \n",
    "  MessagesPlaceholder, \n",
    "  SystemMessagePromptTemplate, \n",
    "  HumanMessagePromptTemplate\n",
    ")\n",
    "memory = ConversationBufferMemory() \n",
    "history=[]\n",
    "     \n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "  ])\n",
    "conversation = ConversationChain(\n",
    "    llm=sm_llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True, \n",
    "    memory=memory\n",
    "  )\n",
    " \n",
    "query = \"What is a Amazon SageMaker Managed Spot Instance?\"    \n",
    "response = conversation.predict(input=query, history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1403d1b-dddb-4439-ad0a-ec096c442f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
